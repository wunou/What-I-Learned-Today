# 아홉번째 배움
###### 2019.04.22 (월)
##### (Coursea) Deep Learning Specialization 공부 중
###### https://www.coursera.org/specializations/deep-learning
##### (Edwith) Bayesian Deep Learning 공부 중
###### https://www.edwith.org/bayesiandeeplearning/joinLectures/14426

---------
<br>

### (Coursea) Deep Learning Specialization
##### ~ Course1 Week3

이전에 따로 기록하지 못했지만, coursera에서 진행되는 Andrew Ng 교수님의 Deep Learning Specialization 강좌를 듣고 있다. 
1주일 무료로 수강할 수 있어서 일단 Course1 Neural Networks and Deep Learning 강좌를 week3까지 들었는데, 언제까지 들을 수 있을 지는 모르겠다..  

<br>
<br>

### (Edwith) Bayesian Deep Learning
##### Random Process

우선 욕심을 버리기로.. 첫 술에 배부르기보단 베이지안 딥러닝 분야의 전체적인 맥락과 스토리를 파악하고 나중에 또 필요하거나 배울 기회가 될 때, 딥-하게 공부해봐야겠다는 생각을 했다. (지금 이 분야에 시간을 많이 쏟을 상황은 아니므로..ㅠ)  
<br>
Random Process의 경우 전에 배운 Random Variable의 확장판으로 보면 될 것 같다. 2차원, 100차원, 무한차원, ... 에서 정의되게 된다. Sequence에 따른 (보통 time sequence) 함수를 정의할 수 있다면 Random Process라 할 수 있다.  
Random Process의 경우 X<sub>t</sub>(w)의 표기를 사용하고 이때 t가 고정되면 Random Variable이 된다.  
Random Process에는 다음과 같은 종류가 있다.
  
* Discrete-time & Discrete-valued
* Discrete-time & Continuous-valued
* Continuous-time & Discrete-valued
* Continuous-time & Continuous-valued  

(이때 입력이 time 이라고 표현되었다고 1차원인 것은 아니다. n차원의 입력이 들어갈 수 있다. output은 1차원.)  
가장 간단한 예가 Brownian motion이다.  

Random Process의 moment는 다음과 같다.  

* mean function
* auto-correlation function, acf
* auto-covariance function, acvf
* cross-covariance function, ccvf  

이때 acf의 경우 머신러닝 논문에 나오는 kernel function과 같은 개념이라고 한다.  
<br>

Random Process는 함수를 다루는데, 이때 나올 수 있는 함수가 너무 많고 또 다양하다. 
그러므로 우리가 다루고자 하는 Random Process를 좁혀보고 싶은데 이런 개념에 의해 등장한 것이 *Stanionarity*이다.  

* If X<sub>t</sub> is strict-sense stationary
  + m<sub>x</sub>(t+a) = m<sub>x</sub>(t)
  + R<sub>x</sub>(t+a, s+a) = R<sub>x</sub>(t, s)
  + C<sub>x</sub>(t+a, s+a) = C<sub>x</sub>(t, s)
* If X<sub>t</sub> is wide-sense stationary
  + m<sub>x</sub>(t+a) = m<sub>x</sub>(t)
  + R<sub>x</sub>(t+a, s+a) = R<sub>x</sub>(t, s)  
  
  
이는 즉 shift-invariant 하다는 것인데, 다음과 같은 성격을 가진다.  

* m<sub>x</sub>(t) = m<sub>x</sub>
* R<sub>x</sub>(t, s) = R<sub>x</sub>(t-s) = R<sub>x</sub>(a)
* C<sub>x</sub>(t, s) = C<sub>x</sub>(t-s) = C<sub>x</sub>(a)
  
즉, 시간 차이가 적으면 함수의 큰 차이가 없고 시간 차이가 많이 나면 함수의 큰 차이가 있을 수 있는, 부드러운 Random Process를 구축할 수 있게 된다.  
이때 아까 언급한 acf가 kernel funcion으로 사용된다는 부분에 대해 이해해볼 수 있다. 
시간 차이가 작을 경우 Random Process의 출력값의 차이가 작을 테고, 즉 acf의 값이 커질 것이다. 
반대로 시간 차이가 클 경우는 출력값의 차이가 커지고, 즉 acf의 값이 작아질 텐데 이러한 특성이 kernel function과 일치한다고 볼 수 있다.  
