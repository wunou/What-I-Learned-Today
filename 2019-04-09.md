## 첫번째 배움
###### 2019.04.09 (화)
-----

### WordPiece tokenizer
##### "Language Independent, Universal Tokenizer" 

<br>

* 단어를 한정적인 유닛(finite subword units)으로 표현한다.
  + (단어의 개수가 늘어나면 계산 비용이 증가, 단어의 개수를 제한하면 미등록 단어 문제 발생하는 문제가 있다.)

<br>

* 자주 등장하는 단어를 units으로 이용한다. 즉, 언어학적 지식과 학습 데이터 없이 units을 정할 수 있다.
  + 개막공연의 경우 개막-, 공연- 의 빈도가 개막공연의 빈도보다 높기 때문에 개막공연을 unit으로 이용하지 않는다. 원 논문(Sennrich et al., 2015) 예시는 아래와 같다.
    - makers, over는 모두 자주 사용되는 단어이므로 units로 이용한고, Jet의 경우 자주 사용되지 않으므로 subword units인 J와 et로 나눈다. 그리고 단어의 시작에는 underbar("\_")를 붙여준다.
      
       **Word:** Jet makers feud over seat width with big orders at stake  
       **WordPieces:** \_J et \_makers \_fe ud \_over \_seat \_width \_with \_big \_orders \_at \_stake
      
  + BPE(Byte Pair Encoding)을 통해 데이터를 압축한다. 빈도수가 높은 최장길이의 substring을 하나의 unit으로 만들어 bit을 아낀다. 아래는 low 5번, lower 2번, newest 6번, widest 3번이 나왔을 때의 예시이다. 
      
      ```python
      # Code from 
      # Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015)

      import re, collections

      def get_stats(vocab):
          pairs = collections.defaultdict(int)
          for word, freq in vocab.items():
              symbols = word.split()
              for i in range(len(symbols)-1):
                  pairs[symbols[i],symbols[i+1]] += freq
          return pairs

      def merge_vocab(pair, v_in):
          v_out = {}
          bigram = re.escape(' '.join(pair))
          p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
          for word in v_in:
              w_out = p.sub(''.join(pair), word)
              v_out[w_out] = v_in[word]
          return v_out

      vocab = {'l o w </w>' : 5,
               'l o w e r </w>' : 2,
               'n e w e s t </w>':6,
               'w i d e s t </w>':3
               }

      num_merges = 10

      for i in range(num_merges):
          pairs = get_stats(vocab)
          best = max(pairs, key=pairs.get)
          vocab = merge_vocab(best, vocab)
          print(best)

      print(vocab)
      ```
      
      ```python
      # 실행 결과
      
      ('e', 's')
      ('es', 't')
      ('est', '</w>')
      ('l', 'o')
      ('lo', 'w')
      ('n', 'e')
      ('ne', 'w')
      ('new', 'est</w>')
      ('low', '</w>')
      ('w', 'i')
      
      vocab = {
         'low</w>': 5, 
         'low e r </w>': 2, 
         'newest</w>': 6, 
         'wi d est</w>': 3
        }
      ```
