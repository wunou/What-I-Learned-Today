# 번째 배움
###### 2019.04.16 (화)
##### XGBoost 공부 중...(1)
###### https://ishuca.tistory.com/388

-----
### (XGBoost) Decision Tree, Random Forest
<br>

(ratsgo 블로그 확인 후 추가 필요)  

XGBoost의 논문 내용 공부 및 이해를 위해 근간이 되는 Decision Tree와 Random Forest 내용을 복습(? 처음 보는 것은 아니었으나 새롭게 공부한 기분이다.)하였다.  
Decision Tree의 경우 말 그대로 결정을 지어주는 root node to leaf nodes 의 구조를 가지는 네트워크이다. 
Random Forest는 말 그대로 숲, Decision Tree들이 모여있는 숲인데, 그 중 Random하게 Tree를 구성하는 네트워크이다. 

<br>

### (XGBoost) Bias-Variance Tradeoff
##### https://bywords.tistory.com/entry/%EB%B2%88%EC%97%AD-%EC%9C%A0%EC%B9%98%EC%9B%90%EC%83%9D%EB%8F%84-%EC%9D%B4%ED%95%B4%ED%95%A0-%EC%88%98-%EC%9E%88%EB%8A%94-biasvariance-tradeoff
<br>

XGBoost에만 해당되는 내용도 아니고, XGBoost와 직접적인 연관이 깊은 내용은 아니지만, 공부 중에 짚고 넘어갈 기회가 생겨 간단하게나마 정리를 하고 넘어가려 한다.  
<br>
  
![](https://t1.daumcdn.net/cfile/tistory/261FE83B562DFB681E)
  
<br>
> ***Error = noise + bias + variance***  
* noise: 데이터가 가지는 본질적인 한계치 (noise: irreducible error / bias, variance: reducible error)
* **bias(Underfitting):** 데이터 셋 내의 모든 정보를 고려하지 않음으로 인해 지속적으로 잘못된 방향 또는 특성들을 학습하는 경향을 말한다. 
* **variance(Overfitting):** 데이터 셋 내의 에러나 노이즈까지 고려하는 *highly flexible models*에 데이터를 fitting시킴으로써, 실제 현상과 관계없는 random한 것들까지 학습하는 경향을 말한다.
<br>
  
bias와 variance는 이러한 *경향*을 의미할 뿐 아니라, 실제로 이 정도를 *측정*하기 위해서도 사용된다.
  
* **bias**: 트레이닝 데이터를 바꿈에 따라서 알고리즘의 평균 정확도가 얼마나 많이 변하는지
* **variance**: 특정 입력 데이터에 대해 알고리즘이 얼마나 민감한지
<br>
  
이때 **Bias-Variance Tradeoff**란 모델을 만드는 데 있어 reducible error인 bias와 variance가 시소관계에 있다는 것이다. 
즉, bias를 낮추기 위해 모델을 단순하게 구성하면 variance가 높아지게 되고, 반대로 variance를 낮추기 위해 모델을 복잡하게 구성하면 bias가 높아지게 되는 것이다.  
다음 그림이 그 상황을 잘 보여준다.  
<br>

![](https://t1.daumcdn.net/cfile/tistory/23635241562E03D316)

<br>

이상적인 모델은 데이터의 규칙성을 잘 잡아내어 *정확하면서도 다른 데이터가 들어왔을 때도 잘 일반화할 수 있는* 모델일 것이다. 
하지만 실제 상황에서 두 가지를 동시에 만족하는 것은 불가능하다. 
학습 데이터가 아닌 *실제 데이터*에서 좋은 성능을 내는 데는 이런 **tradeoff**가 생길 수밖에 없고 이를 **Bias-Variance Tradeoff**라 한다. 

<br>
<br>

-------
## TO DO
1. matplotlib 공부 (4/10~)
2. pandas 공부 (4/12~) (진행 중)
3. **\*BERT 적용 방법 생각**
