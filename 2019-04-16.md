# 다섯번째 배움
###### 2019.04.16 (화)
##### XGBoost 공부 중...(1)
###### https://ishuca.tistory.com/388
#####
-----
### (XGBoost) Decision Tree, Random Forest
<br>

(ratsgo 블로그 확인 후 추가 필요)  

XGBoost의 논문 내용 공부 및 이해를 위해 근간이 되는 Decision Tree와 Random Forest 내용을 복습(? 처음 보는 것은 아니었으나 새롭게 공부한 기분이다.)하였다.  
Decision Tree의 경우 말 그대로 결정을 지어주는 root node to leaf nodes 의 구조를 가지는 네트워크이다. 
Random Forest는 말 그대로 숲, Decision Tree들이 모여있는 숲인데, 그 중 Random하게 Tree를 구성하는 네트워크이다. 

<br>

### (XGBoost) Bias-Variance Tradeoff
##### https://bywords.tistory.com/entry/%EB%B2%88%EC%97%AD-%EC%9C%A0%EC%B9%98%EC%9B%90%EC%83%9D%EB%8F%84-%EC%9D%B4%ED%95%B4%ED%95%A0-%EC%88%98-%EC%9E%88%EB%8A%94-biasvariance-tradeoff
<br>

XGBoost에만 해당되는 내용도 아니고, XGBoost와 직접적인 연관이 깊은 내용은 아니지만, 공부 중에 짚고 넘어갈 기회가 생겨 간단하게나마 정리를 하고 넘어가려 한다.  
<br>
  
> ***Error = noise + bias + variance***  
* noise: 데이터가 가지는 본질적인 한계치 (noise: irreducible error / bias, variance: reducible error)
* **bias(Underfitting):** 데이터 셋 내의 모든 정보를 고려하지 않음으로 인해 지속적으로 잘못된 방향 또는 특성들을 학습하는 경향을 말한다. 
* **variance(Overfitting):** 데이터 셋 내의 에러나 노이즈까지 고려하는 *highly flexible models*에 데이터를 fitting시킴으로써, 실제 현상과 관계없는 random한 것들까지 학습하는 경향을 말한다.
  
bias와 variance는 이러한 *경향*을 의미할 뿐 아니라, 실제로 이 정도를 *측정*하기 위해서도 사용된다.
  
* bias: 트레이닝 데이터를 바꿈에 따라서 알고리즘의 평균 정확도가 얼마나 많이 변하는지
* variance: 특정 입력 데이터에 대해 알고리즘이 얼마나 민감한지
  


<br>
<br>

-------
## TO DO
1. matplotlib 공부 (4/10~)
2. pandas 공부 (4/12~) (진행 중)
3. **\*BERT 적용 방법 생각**
